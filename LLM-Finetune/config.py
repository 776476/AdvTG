"""
LLM-Finetuneé˜¶æ®µé…ç½®ç®¡ç†
ç»Ÿä¸€ç®¡ç†Llama-3-8Bå¾®è°ƒçš„æ‰€æœ‰å‚æ•°å’Œè®¾ç½®
"""
import os
import time
import torch
from typing import Dict, Any, Optional


class LLMConfig:
    """LLMå¾®è°ƒé…ç½®ç®¡ç†ç±»"""
    
    def __init__(self, llm_gpu_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMé…ç½®
        
        Args:
            llm_gpu_config: ä»Žå…¨å±€å¤šGPUé…ç½®èŽ·å–çš„LLMé˜¶æ®µé…ç½®
        """
        self.llm_gpu_config = llm_gpu_config
        self.gpu_count = llm_gpu_config.get('gpu_count', 1)
        self._setup_base_config()
        self._setup_model_config()
        self._setup_training_config()
        self._setup_data_config()
        self._setup_environment_config()
    
    def _setup_base_config(self):
        """åŸºç¡€é…ç½®è®¾ç½®"""
        # é¡¹ç›®é…ç½®
        self.PROJECT_NAME = "AdvTG-LLM-Finetune"
        self.EXPERIMENT_PREFIX = "AdvTG-LLM-Llama3"
        self.DESCRIPTION = "LLM Fine-tuning stage - Llama-3-8B with LoRA"
        
        # è¾“å‡ºç›®å½•
        self.OUTPUT_DIR = "outputs"
        self.DATASET_DIR = "../dataset"
        
        # çŽ¯å¢ƒé…ç½®
        self.SEED = 3407
        self.USE_CUDA_LAUNCH_BLOCKING = True
    
    def _setup_model_config(self):
        """æ¨¡åž‹é…ç½®è®¾ç½®"""
        # æ¨¡åž‹åŸºç¡€é…ç½®
        self.MODEL_NAME = "unsloth/llama-3-8b-bnb-4bit"
        self.MAX_SEQ_LENGTH = 2048
        self.DTYPE = None  # None for auto detection
        self.LOAD_IN_4BIT = True
        
        # LoRAé…ç½®
        self.LORA_R = 16
        self.LORA_ALPHA = 16
        self.LORA_DROPOUT = 0
        self.LORA_BIAS = "none"
        self.USE_RSLORA = False
        self.LOFTQ_CONFIG = None
        
        # LoRAç›®æ ‡æ¨¡å—
        self.TARGET_MODULES = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ]
        
        # è®­ç»ƒä¼˜åŒ–
        self.USE_GRADIENT_CHECKPOINTING = "unsloth"
        self.RANDOM_STATE = 3407
    
    def _setup_training_config(self):
        """è®­ç»ƒé…ç½®è®¾ç½®"""
        # åŸºç¡€è®­ç»ƒå‚æ•°
        self.LEARNING_RATE = 2e-4
        self.WARMUP_STEPS = 5
        self.MAX_STEPS = 500
        self.WEIGHT_DECAY = 0.01
        self.LR_SCHEDULER_TYPE = "linear"
        
        # ç²¾åº¦è®¾ç½® - ä¸ŽUnslothå…¼å®¹
        self.FP16 = False  # ç¦ç”¨fp16
        self.BF16 = True   # ä½¿ç”¨bf16ï¼Œä¸ŽUnslothæ¨¡åž‹åŒ¹é…
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.OPTIM = "adamw_8bit"
        self.LOGGING_STEPS = 1
        
        # å¤šGPUé…ç½®
        self.PER_DEVICE_BATCH_SIZE = self.llm_gpu_config.get('per_device_batch_size', 4)
        self.GRADIENT_ACCUMULATION_STEPS = self.llm_gpu_config.get('gradient_accumulation_steps', 8)
        self.EFFECTIVE_BATCH_SIZE = self.llm_gpu_config.get('effective_batch_size', 32)
        
        # æ•°æ®å¤„ç†
        self.DATASET_NUM_PROC = min(8, self.gpu_count * 2)
        self.DATALOADER_NUM_WORKERS = 0  # é¿å…å¤šè¿›ç¨‹å†²çª
        self.PACKING = False
        
        # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½® - è®©Unslothå¤„ç†
        self.LOCAL_RANK = -1
        self.DDP_BACKEND = None
    
    def _setup_data_config(self):
        """æ•°æ®é…ç½®è®¾ç½®"""
        self.TRAIN_DATA_PATH = os.path.join(self.DATASET_DIR, "llm_train.json")
        self.VAL_DATA_PATH = os.path.join(self.DATASET_DIR, "val.json")
        self.DATA_SHUFFLE_SEED = 42
        
        # Alpacaæç¤ºè¯æ¨¡æ¿
        self.ALPACA_PROMPT = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""
        
        # æŒ‡ä»¤æ¨¡æ¿
        self.MALICIOUS_INSTRUCTION = "Follow these tips to generate malicious http traffic"
        self.BENIGN_INSTRUCTION = "Follow these tips to generate benign http traffic"
    
    def _setup_environment_config(self):
        """çŽ¯å¢ƒå˜é‡é…ç½®"""
        self.ENVIRONMENT_VARS = {
            # Hugging Faceé•œåƒè®¾ç½®
            "HF_ENDPOINT": "https://hf-mirror.com",
            "HUGGINGFACE_HUB_ENDPOINT": "https://hf-mirror.com",
            "HF_HUB_ENDPOINT": "https://hf-mirror.com",
            "HUGGINGFACE_HUB_URL": "https://hf-mirror.com",
            
            # ç¦ç”¨tokenizerså¹¶è¡Œå¤„ç†
            "TOKENIZERS_PARALLELISM": "false",
            
            # ç¦ç”¨wandbï¼Œå¯ç”¨swanlab
            "WANDB_DISABLED": "true",
            "WANDB_MODE": "disabled",
            "WANDB_SILENT": "true",
            
            # CUDAè®¾ç½®
            "CUDA_LAUNCH_BLOCKING": "1" if self.USE_CUDA_LAUNCH_BLOCKING else "0",
            
            # NCCLè®¾ç½® - å¤šGPUè®­ç»ƒ
            "NCCL_P2P_DISABLE": "1",
            "NCCL_IB_DISABLE": "1",
            "NCCL_DEBUG": "INFO",
            "NCCL_SOCKET_IFNAME": "lo"
        }
    
    def setup_environment(self):
        """è®¾ç½®çŽ¯å¢ƒå˜é‡"""
        for key, value in self.ENVIRONMENT_VARS.items():
            os.environ[key] = value
        
        print("ðŸŒ Environment variables configured for LLM training")
        print(f"   - Hugging Face endpoint: {os.environ.get('HF_ENDPOINT')}")
        print(f"   - CUDA launch blocking: {os.environ.get('CUDA_LAUNCH_BLOCKING')}")
        print(f"   - Multi-GPU NCCL configured: âœ…")
    
    def get_experiment_name(self) -> str:
        """ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å®žéªŒåç§°"""
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        return f"{self.EXPERIMENT_PREFIX}-{timestamp}"
    
    def get_swanlab_config(self) -> Dict[str, Any]:
        """èŽ·å–SwanLabé…ç½®"""
        return {
            # æ•°å€¼ç±»åž‹é…ç½® - SwanLabå…¼å®¹
            "model_version": 3.8,  # Llama-3-8Bç‰ˆæœ¬
            "max_seq_length": self.MAX_SEQ_LENGTH,
            "learning_rate": self.LEARNING_RATE,
            "lora_r": self.LORA_R,
            "lora_alpha": self.LORA_ALPHA,
            "target_modules_count": len(self.TARGET_MODULES),
            
            # å¤šGPUé…ç½®ä¿¡æ¯
            "gpu_count": self.gpu_count,
            "per_device_batch_size": self.PER_DEVICE_BATCH_SIZE,
            "gradient_accumulation_steps": self.GRADIENT_ACCUMULATION_STEPS,
            "total_effective_batch_size": self.EFFECTIVE_BATCH_SIZE,
            "multi_gpu_training": 1 if self.gpu_count > 1 else 0,
            
            # è®­ç»ƒé…ç½®
            "max_steps": self.MAX_STEPS,
            "warmup_steps": self.WARMUP_STEPS,
            "weight_decay": self.WEIGHT_DECAY,
            "bf16_enabled": 1 if self.BF16 else 0,
            "use_4bit_quantization": 1 if self.LOAD_IN_4BIT else 0
        }
    
    def get_training_arguments(self) -> Dict[str, Any]:
        """èŽ·å–è®­ç»ƒå‚æ•°é…ç½®"""
        return {
            "per_device_train_batch_size": self.PER_DEVICE_BATCH_SIZE,
            "gradient_accumulation_steps": self.GRADIENT_ACCUMULATION_STEPS,
            "warmup_steps": self.WARMUP_STEPS,
            "max_steps": self.MAX_STEPS,
            "learning_rate": self.LEARNING_RATE,
            "fp16": self.FP16,
            "bf16": self.BF16,
            "logging_steps": self.LOGGING_STEPS,
            "optim": self.OPTIM,
            "weight_decay": self.WEIGHT_DECAY,
            "lr_scheduler_type": self.LR_SCHEDULER_TYPE,
            "seed": self.SEED,
            "output_dir": self.OUTPUT_DIR,
            "local_rank": self.LOCAL_RANK,
            "ddp_backend": self.DDP_BACKEND,
            "dataloader_num_workers": self.DATALOADER_NUM_WORKERS
        }
    
    def get_lora_config(self) -> Dict[str, Any]:
        """èŽ·å–LoRAé…ç½®"""
        return {
            "r": self.LORA_R,
            "target_modules": self.TARGET_MODULES,
            "lora_alpha": self.LORA_ALPHA,
            "lora_dropout": self.LORA_DROPOUT,
            "bias": self.LORA_BIAS,
            "use_gradient_checkpointing": self.USE_GRADIENT_CHECKPOINTING,
            "random_state": self.RANDOM_STATE,
            "use_rslora": self.USE_RSLORA,
            "loftq_config": self.LOFTQ_CONFIG
        }
    
    def get_model_config(self) -> Dict[str, Any]:
        """èŽ·å–æ¨¡åž‹é…ç½®"""
        return {
            "model_name": self.MODEL_NAME,
            "max_seq_length": self.MAX_SEQ_LENGTH,
            "dtype": self.DTYPE,
            "load_in_4bit": self.LOAD_IN_4BIT
        }
    
    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(self.OUTPUT_DIR, exist_ok=True)
        print(f"ðŸ“ Output directory created: {self.OUTPUT_DIR}")
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("ðŸ”§ LLM-Finetune Configuration Summary")
        print("="*60)
        print(f"ðŸ“Š Project: {self.PROJECT_NAME}")
        print(f"ðŸ¤– Model: {self.MODEL_NAME}")
        print(f"ðŸ“ Max sequence length: {self.MAX_SEQ_LENGTH}")
        print(f"ðŸŽ¯ LoRA rank: {self.LORA_R}")
        print(f"âš¡ Learning rate: {self.LEARNING_RATE}")
        print(f"ðŸ“ˆ Max steps: {self.MAX_STEPS}")
        print(f"ðŸ’¾ Use 4-bit: {self.LOAD_IN_4BIT}")
        print(f"ðŸ”¢ BF16 precision: {self.BF16}")
        print(f"ðŸš€ GPU count: {self.gpu_count}")
        print(f"ðŸ“¦ Per device batch size: {self.PER_DEVICE_BATCH_SIZE}")
        print(f"ðŸ”„ Gradient accumulation: {self.GRADIENT_ACCUMULATION_STEPS}")
        print(f"ðŸ“Š Effective batch size: {self.EFFECTIVE_BATCH_SIZE}")
        print("="*60)
    
    def setup_device_and_gpu_info(self) -> torch.device:
        """è®¾ç½®è®¾å¤‡å¹¶æ‰“å°GPUä¿¡æ¯"""
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        gpu_count = torch.cuda.device_count()
        
        print(f"ðŸŽ® Primary device: {device}")
        print(f"ðŸš€ Total GPU count: {gpu_count}")
        
        if gpu_count > 1:
            print(f"ðŸ”¥ Multi-GPU training enabled with {gpu_count} GPUs")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        
        return device
