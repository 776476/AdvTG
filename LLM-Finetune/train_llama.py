"""
LLM-Finetuneé˜¶æ®µä¸»è®­ç»ƒè„šæœ¬ - é‡æ„åçš„ç®€åŒ–ç‰ˆæœ¬
ä½¿ç”¨é…ç½®æ–‡ä»¶ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å‚æ•°ï¼Œæ”¯æŒLlama-3-8B + LoRAå¾®è°ƒ
"""
import os
import sys
import torch

# ç¦ç”¨wandb - å¿…é¡»åœ¨å¯¼å…¥transformersä¹‹å‰è®¾ç½®
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "disabled"

# å¯¼å…¥å…¨å±€å¤šGPUé…ç½®
sys.path.append('..')
from multi_gpu_config import AdvTGMultiGPUConfig

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from config import LLMConfig
from data_utils import create_data_processor
from swanlab_utils import create_swanlab_manager, create_swanlab_callback


class LLMTrainer:
    """LLMå¾®è°ƒè®­ç»ƒå™¨ç±»"""
    
    def __init__(self):
        self.config = None
        self.global_gpu_config = None
        self.llm_gpu_config = None
        self.swanlab_manager = None
        self.data_processor = None
        self.model = None
        self.tokenizer = None
        self.train_dataset = None
        self.val_dataset = None
    
    def initialize(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        print("ğŸš€ Starting LLM fine-tuning with multi-GPU support...")
        
        # åˆå§‹åŒ–å…¨å±€å¤šGPUé…ç½®
        self.global_gpu_config = AdvTGMultiGPUConfig()
        self.llm_gpu_config = self.global_gpu_config.get_stage_config("LLM")
        
        # åˆ›å»ºLLMé…ç½®
        self.config = LLMConfig(self.llm_gpu_config)
        
        # è®¾ç½®ç¯å¢ƒå’Œç›®å½•
        self.config.setup_environment()
        self.config.create_directories()
        self.config.print_config_summary()
        
        # è®¾ç½®è®¾å¤‡ä¿¡æ¯
        device = self.config.setup_device_and_gpu_info()
        
        # åˆå§‹åŒ–SwanLabç®¡ç†å™¨
        self.swanlab_manager = create_swanlab_manager(self.config)
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_processor = create_data_processor(self.config)
        
        return device
    
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("\nğŸ¤– Loading Llama-3-8B model and tokenizer...")
        
        # å¯¼å…¥Unsloth
        from unsloth import FastLanguageModel
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self.config.get_model_config()
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_config["model_name"],
            max_seq_length=model_config["max_seq_length"],
            dtype=model_config["dtype"],
            load_in_4bit=model_config["load_in_4bit"]
        )
        
        print(f"âœ… Model loaded: {model_config['model_name']}")
        print(f"ğŸ“ Max sequence length: {model_config['max_seq_length']}")
        print(f"ğŸ’¾ 4-bit quantization: {model_config['load_in_4bit']}")
        
        # è®¾ç½®EOS tokenç”¨äºæ•°æ®å¤„ç†
        self.data_processor.set_eos_token(self.tokenizer.eos_token)
    
    def setup_lora_model(self):
        """è®¾ç½®LoRAæ¨¡å‹"""
        print("\nğŸ”§ Setting up LoRA configuration...")
        
        # è·å–LoRAé…ç½®
        lora_config = self.config.get_lora_config()
        
        # åº”ç”¨LoRAé…ç½®
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            **lora_config
        )
        
        print(f"âœ… LoRA applied with rank: {lora_config['r']}")
        print(f"ğŸ¯ Target modules: {len(lora_config['target_modules'])} modules")
        print(f"âš¡ LoRA alpha: {lora_config['lora_alpha']}")
    
    def prepare_datasets(self):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        print("\nğŸ“Š Preparing datasets...")
        
        # åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
        self.train_dataset, self.val_dataset = self.data_processor.load_and_prepare_datasets()
        
        # æ‰“å°æ ·æœ¬æ•°æ®
        self.data_processor.print_sample_data(self.train_dataset, num_samples=2)
        
        return self.train_dataset, self.val_dataset
    
    def create_trainer(self):
        """åˆ›å»ºè®­ç»ƒå™¨"""
        print("\nğŸ‹ï¸ Creating SFT trainer...")
        
        from trl import SFTTrainer
        from transformers import TrainingArguments
        
        # è·å–è®­ç»ƒå‚æ•°
        training_args_config = self.config.get_training_arguments()
        training_args = TrainingArguments(**training_args_config)
        
        # åˆ›å»ºSwanLabå›è°ƒ
        swanlab_callback = create_swanlab_callback(self.swanlab_manager.use_swanlab)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            dataset_text_field="text",
            max_seq_length=self.config.MAX_SEQ_LENGTH,
            dataset_num_proc=self.config.DATASET_NUM_PROC,
            packing=self.config.PACKING,
            callbacks=[swanlab_callback] if self.swanlab_manager.use_swanlab else [],
            args=training_args
        )
        
        print("âœ… SFT trainer created successfully!")
        print(f"ğŸ“¦ Dataset processing workers: {self.config.DATASET_NUM_PROC}")
        print(f"ğŸ“Š Batch size per device: {self.config.PER_DEVICE_BATCH_SIZE}")
        print(f"ğŸ”„ Gradient accumulation steps: {self.config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"ğŸ“ˆ Total effective batch size: {self.config.EFFECTIVE_BATCH_SIZE}")
        
        return trainer
    
    def train_model(self, trainer):
        """æ‰§è¡Œæ¨¡å‹è®­ç»ƒ"""
        print("\nğŸš€ Starting model training...")
        print(f"ğŸ“ˆ Max training steps: {self.config.MAX_STEPS}")
        print(f"ğŸ”¥ Learning rate: {self.config.LEARNING_RATE}")
        print(f"ğŸ¯ Target precision: {'BF16' if self.config.BF16 else 'FP32'}")
        
        # å¼€å§‹è®­ç»ƒ
        trainer_stats = trainer.train()
        
        print("\nâœ… Training completed!")
        
        # è®°å½•è®­ç»ƒç»“æœåˆ°SwanLab
        self.swanlab_manager.log_training_completion(trainer_stats)
        
        return trainer_stats
    
    def save_model(self, trainer):
        """ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹"""
        print("\nğŸ’¾ Saving trained model...")
        
        try:
            # ä¿å­˜æ¨¡å‹å’Œtokenizer
            trainer.save_model(self.config.OUTPUT_DIR)
            self.tokenizer.save_pretrained(self.config.OUTPUT_DIR)
            
            print(f"âœ… Model saved to: {self.config.OUTPUT_DIR}")
            
        except Exception as e:
            print(f"âš ï¸  Model saving failed: {e}")
    
    def finalize_training(self):
        """å®Œæˆè®­ç»ƒå¹¶æ¸…ç†èµ„æº"""
        print("\nğŸ Finalizing training...")
        
        # å®ŒæˆSwanLabå®éªŒ
        self.swanlab_manager.finish()
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU cache cleared")
        
        print("ğŸ‰ LLM fine-tuning pipeline completed successfully!")
    
    def run_training_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            # 1. åˆå§‹åŒ–
            device = self.initialize()
            
            # 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 3. è®¾ç½®LoRA
            self.setup_lora_model()
            
            # 4. å‡†å¤‡æ•°æ®é›†
            self.prepare_datasets()
            
            # 5. åˆ›å»ºè®­ç»ƒå™¨
            trainer = self.create_trainer()
            
            # 6. è®­ç»ƒæ¨¡å‹
            trainer_stats = self.train_model(trainer)
            
            # 7. ä¿å­˜æ¨¡å‹
            self.save_model(trainer)
            
            # 8. å®Œæˆè®­ç»ƒ
            self.finalize_training()
            
            return trainer_stats
            
        except Exception as e:
            print(f"âŒ Training pipeline failed: {e}")
            # ç¡®ä¿SwanLabæ­£ç¡®å…³é—­
            if self.swanlab_manager:
                self.swanlab_manager.finish()
            raise


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ï¼ˆWindowså…¼å®¹ï¼‰
    import multiprocessing as mp
    if hasattr(mp, 'set_start_method'):
        try:
            mp.set_start_method('spawn', force=True)
        except RuntimeError:
            pass  # å·²ç»è®¾ç½®è¿‡äº†
    
    # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒå™¨
    trainer = LLMTrainer()
    trainer_stats = trainer.run_training_pipeline()
    
    return trainer_stats


if __name__ == "__main__":
    # å¯¼å…¥FastLanguageModeléœ€è¦åœ¨mainä¿æŠ¤ä¸‹
    from unsloth import FastLanguageModel
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
