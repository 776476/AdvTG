"""
LLMæ•°æ®å¤„ç†å·¥å…·
å¤„ç†HTTPæ•°æ®é›†å¹¶æ ¼å¼åŒ–ä¸ºAlpacaæ ¼å¼
"""
import json
import os
from typing import Dict, List, Any, Tuple
from datasets import Dataset


class LLMDataProcessor:
    """LLMæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            config: LLMConfigé…ç½®å¯¹è±¡
        """
        self.config = config
        self.alpaca_prompt = config.ALPACA_PROMPT
        self.malicious_instruction = config.MALICIOUS_INSTRUCTION
        self.benign_instruction = config.BENIGN_INSTRUCTION
        self.eos_token = None  # å°†åœ¨tokenizeråŠ è½½åè®¾ç½®
    
    def set_eos_token(self, eos_token: str):
        """è®¾ç½®EOS token"""
        self.eos_token = eos_token
    
    def json_to_string(self, data: Any, indent: int = 0) -> str:
        """
        å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        
        Args:
            data: è¦è½¬æ¢çš„æ•°æ®
            indent: ç¼©è¿›çº§åˆ«
            
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        result = []
        indent_str = ' ' * indent
        
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, (dict, list)):
                    result.append(f'{indent_str}{key}:')
                    result.append(self.json_to_string(value, indent + 1))
                else:
                    result.append(f'{indent_str}{key}: {value}')
        elif isinstance(data, list):
            for item in data:
                result.append(self.json_to_string(item, indent))
        else:
            result.append(f'{indent_str}{data}')
        
        return '\n'.join(result)
    
    def load_json_data(self, file_path: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½JSONæ•°æ®æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½çš„æ•°æ®åˆ—è¡¨
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file not found: {file_path}")
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        print(f"ğŸ“ Loaded {len(data)} samples from {file_path}")
        return data
    
    def format_single_sample(self, item: Dict[str, Any]) -> Dict[str, str]:
        """
        æ ¼å¼åŒ–å•ä¸ªæ ·æœ¬
        
        Args:
            item: åŸå§‹æ•°æ®é¡¹
            
        Returns:
            æ ¼å¼åŒ–åçš„æ•°æ®é¡¹
        """
        # æ„å»ºè¯·æ±‚å†…å®¹
        request_content = (
            item["Request Line"] + "\n" + 
            self.json_to_string(item["Request Headers"]) + "\n\n" + 
            item["Request Body"]
        )
        
        # æ ¹æ®æ ‡ç­¾é€‰æ‹©æŒ‡ä»¤
        instruction = (
            self.malicious_instruction if item["Label"] == "Malicious" 
            else self.benign_instruction
        )
        
        return {
            "text": request_content,
            "instruction": instruction,
            "input": item["Request Line"],
            "output": request_content
        }
    
    def format_data_for_training(self, raw_data: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """
        å°†åŸå§‹æ•°æ®æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼
        
        Args:
            raw_data: åŸå§‹æ•°æ®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„æ•°æ®å­—å…¸
        """
        formatted_data = {
            "text": [],
            "instruction": [],
            "input": [],
            "output": []
        }
        
        for item in raw_data:
            formatted_item = self.format_single_sample(item)
            for key in formatted_data.keys():
                formatted_data[key].append(formatted_item[key])
        
        return formatted_data
    
    def formatting_prompts_func(self, examples: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        å°†æ•°æ®æ ¼å¼åŒ–ä¸ºAlpacaæç¤ºè¯æ ¼å¼
        
        Args:
            examples: æ‰¹é‡æ ·æœ¬æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬åˆ—è¡¨
        """
        instructions = examples["instruction"]
        inputs = examples["input"]
        outputs = examples["output"]
        texts = []
        
        for instruction, input_text, output in zip(instructions, inputs, outputs):
            # æ·»åŠ EOS tokenï¼Œå¦åˆ™ç”Ÿæˆä¼šæ— é™è¿›è¡Œ
            text = self.alpaca_prompt.format(instruction, input_text, output)
            if self.eos_token:
                text += self.eos_token
            texts.append(text)
        
        return {"text": texts}
    
    def create_dataset(self, raw_data: List[Dict[str, Any]], shuffle: bool = True) -> Dataset:
        """
        åˆ›å»ºDatasetå¯¹è±¡
        
        Args:
            raw_data: åŸå§‹æ•°æ®åˆ—è¡¨
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
            
        Returns:
            Datasetå¯¹è±¡
        """
        # æ ¼å¼åŒ–æ•°æ®
        formatted_data = self.format_data_for_training(raw_data)
        
        # åˆ›å»ºDataset
        dataset = Dataset.from_dict(formatted_data)
        
        # æ‰“ä¹±æ•°æ®
        if shuffle:
            dataset = dataset.shuffle(seed=self.config.DATA_SHUFFLE_SEED)
        
        # åº”ç”¨Alpacaæ ¼å¼åŒ–
        dataset = dataset.map(self.formatting_prompts_func, batched=True)
        
        return dataset
    
    def load_and_prepare_datasets(self) -> Tuple[Dataset, Dataset]:
        """
        åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        
        Returns:
            (train_dataset, val_dataset) å…ƒç»„
        """
        print("ğŸ“Š Loading and preparing datasets...")
        
        # åŠ è½½åŸå§‹æ•°æ®
        train_data = self.load_json_data(self.config.TRAIN_DATA_PATH)
        val_data = self.load_json_data(self.config.VAL_DATA_PATH)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = self.create_dataset(train_data, shuffle=True)
        val_dataset = self.create_dataset(val_data, shuffle=True)
        
        print(f"âœ… Datasets prepared:")
        print(f"   - Training samples: {len(train_dataset)}")
        print(f"   - Validation samples: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def print_sample_data(self, dataset: Dataset, num_samples: int = 2):
        """
        æ‰“å°æ ·æœ¬æ•°æ®ç”¨äºæ£€æŸ¥
        
        Args:
            dataset: æ•°æ®é›†
            num_samples: æ‰“å°çš„æ ·æœ¬æ•°é‡
        """
        print(f"\nğŸ“‹ Sample data (first {num_samples} samples):")
        for i in range(min(num_samples, len(dataset))):
            sample = dataset[i]
            print(f"\n--- Sample {i+1} ---")
            print(f"Text length: {len(sample['text'])} chars")
            print(f"Text preview: {sample['text'][:200]}...")
            if len(sample['text']) > 200:
                print("    [truncated]")
        print()


def create_data_processor(config) -> LLMDataProcessor:
    """
    åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
    
    Args:
        config: LLMConfigé…ç½®å¯¹è±¡
        
    Returns:
        LLMDataProcessorå®ä¾‹
    """
    return LLMDataProcessor(config)
