#!/usr/bin/env python3
"""
å…¨å±€å¤šGPUé…ç½®æ¨¡å— - ä¸ºAdvTGä¸‰ä¸ªé˜¶æ®µæä¾›ç»Ÿä¸€çš„å¤šGPUä¼˜åŒ–
æ”¯æŒDLé˜¶æ®µã€LLMé˜¶æ®µå’ŒRLé˜¶æ®µçš„è‡ªé€‚åº”GPUé…ç½®
"""
import os
import torch
import multiprocessing as mp
from typing import Dict, Tuple, Optional

class AdvTGMultiGPUConfig:
    """AdvTGå¤šGPUé…ç½®ç®¡ç†ç±»"""
    
    def __init__(self):
        self.gpu_count = 0
        self.is_initialized = False
        self.stage_configs = {}
        
    def initialize_global_gpu_environment(self) -> bool:
        """åˆå§‹åŒ–å…¨å±€GPUç¯å¢ƒ"""
        if self.is_initialized:
            return True
        
        # æ¸…é™¤å¯èƒ½çš„GPUé™åˆ¶ç¯å¢ƒå˜é‡ï¼Œä»¥ä¾¿æ£€æµ‹æ‰€æœ‰GPU
        if 'CUDA_VISIBLE_DEVICES' in os.environ:
            print(f"ğŸ” æ¸…é™¤ç°æœ‰CUDA_VISIBLE_DEVICESé™åˆ¶: {os.environ['CUDA_VISIBLE_DEVICES']}")
            del os.environ['CUDA_VISIBLE_DEVICES']
            
        # æ£€æµ‹GPUå¯ç”¨æ€§
        self.gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        if self.gpu_count == 0:
            print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            self.is_initialized = True
            return False
            
        print(f"ğŸš€ AdvTGå…¨å±€GPUé…ç½®åˆå§‹åŒ–")
        print(f"æ£€æµ‹åˆ° {self.gpu_count} å¼ GPU:")
        
        # æ˜¾ç¤ºæ‰€æœ‰GPUä¿¡æ¯
        for i in range(self.gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_gb = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"   GPU {i}: {gpu_name} ({memory_gb:.1f} GB)")
        
        # è®¾ç½®å…¨å±€GPUç¯å¢ƒå˜é‡
        if self.gpu_count > 1:
            # è®¾ç½®æ‰€æœ‰GPUå¯è§
            gpu_list = ','.join(str(i) for i in range(self.gpu_count))
            os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list
            
            # å¤šGPUé€šä¿¡é…ç½®ï¼ˆNCCLä¼˜åŒ–ï¼‰
            os.environ['NCCL_P2P_DISABLE'] = '1'
            os.environ['NCCL_IB_DISABLE'] = '1'
            os.environ['NCCL_DEBUG'] = 'INFO'
            os.environ['NCCL_SOCKET_IFNAME'] = 'lo'
            
            # PyTorchå¤šGPUä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            
            print(f"âœ… å¤šGPUç¯å¢ƒé…ç½®å®Œæˆ")
        else:
            print(f"ğŸ“± å•GPUæ¨¡å¼")
            
        # CUDAåŸºç¡€ä¼˜åŒ–
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        
        self.is_initialized = True
        return True
    
    def get_stage_config(self, stage: str) -> Dict:
        """è·å–ç‰¹å®šé˜¶æ®µçš„GPUé…ç½®
        
        Args:
            stage: 'DL', 'LLM', 'RL'
        """
        if not self.is_initialized:
            self.initialize_global_gpu_environment()
            
        if stage in self.stage_configs:
            return self.stage_configs[stage]
            
        # æ ¹æ®ä¸åŒé˜¶æ®µè®¡ç®—æœ€ä¼˜é…ç½®
        config = self._calculate_stage_config(stage)
        self.stage_configs[stage] = config
        
        print(f"\nğŸ”§ {stage}é˜¶æ®µå¤šGPUé…ç½®:")
        print(f"   - GPUæ•°é‡: {config['gpu_count']}")
        print(f"   - æ¯è®¾å¤‡batch size: {config['per_device_batch_size']}")
        print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config['gradient_accumulation_steps']}")
        print(f"   - æ€»æœ‰æ•ˆbatch size: {config['effective_batch_size']}")
        print(f"   - æ•°æ®åŠ è½½workeræ•°: {config['dataloader_num_workers']}")
        print(f"   - æ··åˆç²¾åº¦: {config['enable_mixed_precision']}")
        
        return config
    
    def _calculate_stage_config(self, stage: str) -> Dict:
        """æ ¹æ®è®­ç»ƒé˜¶æ®µè®¡ç®—æœ€ä¼˜GPUé…ç½®"""
        cpu_count = mp.cpu_count()
        
        if self.gpu_count == 0:
            return self._get_cpu_config(stage)
            
        if stage == "DL":
            # DLé˜¶æ®µï¼šBERT + è‡ªå®šä¹‰æ¨¡å‹ï¼Œå†…å­˜éœ€æ±‚ä¸­ç­‰
            if self.gpu_count >= 4:
                per_device_batch_size = 16
                gradient_accumulation = 2
                num_workers = min(8, cpu_count // 2)
            elif self.gpu_count >= 2:
                per_device_batch_size = 24
                gradient_accumulation = 4
                num_workers = min(6, cpu_count // 2)
            else:
                per_device_batch_size = 32
                gradient_accumulation = 8
                num_workers = min(4, cpu_count // 2)
                
        elif stage == "LLM":
            # LLMé˜¶æ®µï¼šLlama-3-8Bï¼Œå†…å­˜éœ€æ±‚æœ€é«˜
            if self.gpu_count >= 8:
                per_device_batch_size = 2
                gradient_accumulation = 8
                num_workers = min(16, cpu_count)
            elif self.gpu_count >= 4:
                per_device_batch_size = 4
                gradient_accumulation = 16
                num_workers = min(12, cpu_count // 2)
            elif self.gpu_count >= 2:
                per_device_batch_size = 6
                gradient_accumulation = 24
                num_workers = min(8, cpu_count // 2)
            else:
                per_device_batch_size = 8
                gradient_accumulation = 64
                num_workers = min(4, cpu_count // 2)
                
        elif stage == "RL":
            # RLé˜¶æ®µï¼šPPOè®­ç»ƒï¼Œbatch sizeè¾ƒå°ä½†éœ€è¦å¿«é€Ÿå“åº”
            if self.gpu_count >= 4:
                per_device_batch_size = 4
                gradient_accumulation = 2
                num_workers = min(6, cpu_count // 4)
            elif self.gpu_count >= 2:
                per_device_batch_size = 6
                gradient_accumulation = 4
                num_workers = min(4, cpu_count // 4)
            else:
                per_device_batch_size = 8
                gradient_accumulation = 8
                num_workers = min(2, cpu_count // 4)
        else:
            # é»˜è®¤é…ç½®
            per_device_batch_size = 8
            gradient_accumulation = 8
            num_workers = min(4, cpu_count // 4)
            
        effective_batch_size = per_device_batch_size * gradient_accumulation * self.gpu_count
        
        return {
            "gpu_count": self.gpu_count,
            "per_device_batch_size": per_device_batch_size,
            "gradient_accumulation_steps": gradient_accumulation,
            "effective_batch_size": effective_batch_size,
            "dataloader_num_workers": num_workers,
            "enable_mixed_precision": True,
            "enable_gradient_checkpointing": stage in ["LLM", "RL"],  # LLMå’ŒRLå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            "dataloader_pin_memory": True,
            "ddp_find_unused_parameters": False,
            "remove_unused_columns": False,
            "stage": stage
        }
    
    def _get_cpu_config(self, stage: str) -> Dict:
        """CPUæ¨¡å¼é…ç½®"""
        cpu_count = mp.cpu_count()
        
        return {
            "gpu_count": 0,
            "per_device_batch_size": 8 if stage != "LLM" else 4,
            "gradient_accumulation_steps": 16 if stage == "LLM" else 8,
            "effective_batch_size": 128 if stage != "LLM" else 64,
            "dataloader_num_workers": min(4, cpu_count // 2),
            "enable_mixed_precision": False,
            "enable_gradient_checkpointing": True,
            "dataloader_pin_memory": False,
            "ddp_find_unused_parameters": False,
            "remove_unused_columns": False,
            "stage": stage
        }
    
    def get_training_arguments_kwargs(self, stage: str, additional_kwargs: Optional[Dict] = None) -> Dict:
        """è·å–TrainingArgumentsçš„å…³é”®å­—å‚æ•°"""
        config = self.get_stage_config(stage)
        
        kwargs = {
            "per_device_train_batch_size": config["per_device_batch_size"],
            "gradient_accumulation_steps": config["gradient_accumulation_steps"],
            "dataloader_num_workers": config["dataloader_num_workers"],
            "dataloader_pin_memory": config["dataloader_pin_memory"],
            "ddp_find_unused_parameters": config["ddp_find_unused_parameters"],
            "remove_unused_columns": config["remove_unused_columns"],
        }
        
        # æ··åˆç²¾åº¦é…ç½®
        if config["enable_mixed_precision"] and torch.cuda.is_available():
            kwargs["fp16"] = True
            
        # æ¢¯åº¦æ£€æŸ¥ç‚¹
        if config["enable_gradient_checkpointing"]:
            kwargs["gradient_checkpointing"] = True
            
        # å¤šGPUç‰¹å®šé…ç½®
        if config["gpu_count"] > 1:
            kwargs["ddp_backend"] = "nccl"
            
        # åˆå¹¶é¢å¤–å‚æ•°
        if additional_kwargs:
            kwargs.update(additional_kwargs)
            
        return kwargs
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸš€ AdvTGå¤šGPUé…ç½®æ‘˜è¦")
        print("="*60)
        print(f"æ€»GPUæ•°é‡: {self.gpu_count}")
        
        if self.stage_configs:
            for stage, config in self.stage_configs.items():
                print(f"\n{stage}é˜¶æ®µé…ç½®:")
                print(f"  - æ¯è®¾å¤‡batch size: {config['per_device_batch_size']}")
                print(f"  - æ€»æœ‰æ•ˆbatch size: {config['effective_batch_size']}")
                print(f"  - æ¢¯åº¦ç´¯ç§¯: {config['gradient_accumulation_steps']}")
                print(f"  - æ•°æ®åŠ è½½workers: {config['dataloader_num_workers']}")
        print("="*60)

# å…¨å±€å¤šGPUé…ç½®å®ä¾‹
global_gpu_config = AdvTGMultiGPUConfig()

def get_multi_gpu_config(stage: str) -> Dict:
    """è·å–æŒ‡å®šé˜¶æ®µçš„å¤šGPUé…ç½®
    
    Args:
        stage: 'DL', 'LLM', 'RL'
        
    Returns:
        Dict: åŒ…å«GPUé…ç½®çš„å­—å…¸
    """
    return global_gpu_config.get_stage_config(stage)

def initialize_multi_gpu_for_stage(stage: str) -> Dict:
    """ä¸ºç‰¹å®šé˜¶æ®µåˆå§‹åŒ–å¤šGPUé…ç½®
    
    Args:
        stage: 'DL', 'LLM', 'RL'
        
    Returns:
        Dict: GPUé…ç½®å­—å…¸
    """
    print(f"\nğŸ¯ åˆå§‹åŒ– {stage} é˜¶æ®µå¤šGPUé…ç½®...")
    config = global_gpu_config.get_stage_config(stage)
    
    # è®¾ç½®ç‰¹å®šäºé˜¶æ®µçš„ç¯å¢ƒå˜é‡
    if stage == "LLM":
        # LLMé˜¶æ®µç‰¹æ®Šé…ç½®
        os.environ["WANDB_DISABLED"] = "true"
        os.environ["WANDB_MODE"] = "disabled"
        os.environ["WANDB_SILENT"] = "true"
    elif stage == "RL":
        # RLé˜¶æ®µç‰¹æ®Šé…ç½®
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
    return config

def get_training_arguments_for_stage(stage: str, base_args: Optional[Dict] = None) -> Dict:
    """è·å–ç‰¹å®šé˜¶æ®µçš„TrainingArgumentså‚æ•°"""
    return global_gpu_config.get_training_arguments_kwargs(stage, base_args)

def print_multi_gpu_summary():
    """æ‰“å°å¤šGPUé…ç½®æ‘˜è¦"""
    global_gpu_config.print_config_summary()

if __name__ == "__main__":
    # æµ‹è¯•å¤šGPUé…ç½®
    print("ğŸ§ª æµ‹è¯•AdvTGå¤šGPUé…ç½®...")
    
    # åˆå§‹åŒ–å…¨å±€ç¯å¢ƒ
    global_gpu_config.initialize_global_gpu_environment()
    
    # æµ‹è¯•ä¸‰ä¸ªé˜¶æ®µçš„é…ç½®
    for stage in ["DL", "LLM", "RL"]:
        config = get_multi_gpu_config(stage)
        print(f"\n{stage}é˜¶æ®µæµ‹è¯•å®Œæˆ")
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_multi_gpu_summary()
