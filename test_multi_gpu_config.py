#!/usr/bin/env python3
"""
AdvTGå…¨å±€å¤šGPUé…ç½®æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸‰ä¸ªé˜¶æ®µçš„GPUé…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
sys.path.append('.')

def test_multi_gpu_config():
    """æµ‹è¯•å…¨å±€å¤šGPUé…ç½®"""
    print("ğŸ§ª æµ‹è¯•AdvTGå…¨å±€å¤šGPUé…ç½®...")
    print("=" * 60)
    
    try:
        from multi_gpu_config import (
            initialize_multi_gpu_for_stage, 
            get_multi_gpu_config, 
            get_training_arguments_for_stage,
            print_multi_gpu_summary
        )
        
        print("âœ… æˆåŠŸå¯¼å…¥å¤šGPUé…ç½®æ¨¡å—")
        
        # æµ‹è¯•ä¸‰ä¸ªé˜¶æ®µçš„é…ç½®
        stages = ["DL", "LLM", "RL"]
        
        for stage in stages:
            print(f"\nğŸ”§ æµ‹è¯• {stage} é˜¶æ®µé…ç½®:")
            try:
                # åˆå§‹åŒ–é˜¶æ®µé…ç½®
                config = initialize_multi_gpu_for_stage(stage)
                
                # è·å–TrainingArgumentsé…ç½®
                training_args_config = get_training_arguments_for_stage(stage)
                
                print(f"   âœ… {stage}é˜¶æ®µé…ç½®æˆåŠŸ")
                print(f"   - GPUæ•°é‡: {config['gpu_count']}")
                print(f"   - æ¯è®¾å¤‡batch size: {config['per_device_batch_size']}")
                print(f"   - æ€»æœ‰æ•ˆbatch size: {config['effective_batch_size']}")
                print(f"   - TrainingArgumentså‚æ•°æ•°é‡: {len(training_args_config)}")
                
            except Exception as e:
                print(f"   âŒ {stage}é˜¶æ®µé…ç½®å¤±è´¥: {e}")
                return False
        
        # æ‰“å°é…ç½®æ‘˜è¦
        print_multi_gpu_summary()
        
        print("\n" + "=" * 60)
        print("âœ… å…¨å±€å¤šGPUé…ç½®æµ‹è¯•é€šè¿‡!")
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gpu_detection():
    """æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½"""
    print("\nğŸ” GPUæ£€æµ‹æµ‹è¯•:")
    try:
        import torch
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"âœ… æ£€æµ‹åˆ° {gpu_count} å¼ GPU:")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                memory_gb = torch.cuda.get_device_properties(i).total_memory / 1e9
                print(f"   GPU {i}: {gpu_name} ({memory_gb:.1f} GB)")
            
            return gpu_count >= 1
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°CUDA GPU")
            return False
            
    except ImportError:
        print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU")
        return False
    except Exception as e:
        print(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ AdvTGå¤šGPUé…ç½®å®Œæ•´æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•GPUæ£€æµ‹
    gpu_available = test_gpu_detection()
    
    # æµ‹è¯•å¤šGPUé…ç½®
    config_success = test_multi_gpu_config()
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"   GPUå¯ç”¨æ€§: {'âœ… é€šè¿‡' if gpu_available else 'âš ï¸  æœªæ£€æµ‹åˆ°GPU'}")
    print(f"   å¤šGPUé…ç½®: {'âœ… é€šè¿‡' if config_success else 'âŒ å¤±è´¥'}")
    
    if config_success:
        print("\nğŸ‰ AdvTGå¤šGPUé…ç½®å·²å‡†å¤‡å°±ç»ª!")
        print("   - DLé˜¶æ®µ: BERT + è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒ")
        print("   - LLMé˜¶æ®µ: Llama-3-8B + LoRAå¾®è°ƒ")
        print("   - RLé˜¶æ®µ: PPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
        return True
    else:
        print("\nâŒ å¤šGPUé…ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
