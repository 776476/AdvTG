import os
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import time

# å¯¼å…¥å…¨å±€å¤šGPUé…ç½®
import sys
sys.path.append('..')
from multi_gpu_config import initialize_multi_gpu_for_stage, get_training_arguments_for_stage

# Set Hugging Face mirror BEFORE importing transformers
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HUGGINGFACE_HUB_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HUGGINGFACE_HUB_URL"] = "https://hf-mirror.com"

# Disable tokenizers parallelism to avoid fork warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from transformers import TrainingArguments, TrainerCallback, EarlyStoppingCallback

from models import CNNLSTMClassifier, TextCNNClassifier, DNNClassifier, DeepLog
from data_processing import load_data, prepare_dataset, load_tokenizer, SimpleTokenizer
from training import train_transformer_model, train_custom_model

# vLLMé£æ ¼çš„DLå¹¶è¡Œè®­ç»ƒé…ç½® - ç°åœ¨ä½¿ç”¨å…¨å±€å¤šGPUé…ç½®
ENABLE_VLLM_STYLE_PARALLEL = True   # å¯ç”¨vLLMé£æ ¼å¹¶è¡Œä¼˜åŒ–
ENABLE_TENSOR_PARALLEL = True       # å¯ç”¨å¼ é‡å¹¶è¡Œï¼ˆå¤šGPUï¼‰
ENABLE_DATA_PARALLEL = True         # å¯ç”¨æ•°æ®å¹¶è¡Œå¤„ç†
MAX_PARALLEL_WORKERS = min(6, mp.cpu_count())  # å¢åŠ å¹¶è¡Œå·¥ä½œè¿›ç¨‹

# åˆ›å»ºSwanLabå›è°ƒå‡½æ•°ç”¨äºå®æ—¶è®°å½•DLè®­ç»ƒè¿‡ç¨‹
class DLSwanLabCallback(TrainerCallback):
    def __init__(self, use_swanlab=False, model_name=""):
        self.use_swanlab = use_swanlab
        self.model_name = model_name
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """åœ¨æ¯æ¬¡æ—¥å¿—è®°å½•æ—¶è°ƒç”¨"""
        if self.use_swanlab and logs:
            try:
                import swanlab
                # è®°å½•æŸå¤±å’Œå­¦ä¹ ç‡ - ä½¿ç”¨å‘½åç©ºé—´åˆ†ç»„
                log_dict = {}
                if 'loss' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/train_loss'] = logs['loss']
                    log_dict[f'ModelComparison/TransformerModels/{self.model_name}_train_loss'] = logs['loss']
                if 'learning_rate' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/learning_rate'] = logs['learning_rate']
                if 'epoch' in logs:
                    log_dict['epoch'] = logs['epoch']  # å…¨å±€epoch
                    log_dict[f'TransformerModels/{self.model_name}/epoch'] = logs['epoch']
                if 'eval_loss' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/eval_loss'] = logs['eval_loss']
                    log_dict[f'ModelComparison/TransformerModels/{self.model_name}_eval_loss'] = logs['eval_loss']
                if 'eval_accuracy' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/accuracy'] = logs['eval_accuracy']
                    log_dict[f'ModelComparison/TransformerModels/{self.model_name}_accuracy'] = logs['eval_accuracy']
                if 'eval_precision' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/precision'] = logs['eval_precision']
                if 'eval_recall' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/recall'] = logs['eval_recall']
                if 'eval_f1' in logs:
                    log_dict[f'TransformerModels/{self.model_name}/f1'] = logs['eval_f1']
                    log_dict[f'ModelComparison/TransformerModels/{self.model_name}_f1'] = logs['eval_f1']
                
                # æ·»åŠ stepä¿¡æ¯
                log_dict['step'] = state.global_step
                
                if log_dict:
                    swanlab.log(log_dict)
                    print(f"ğŸ“Š {self.model_name} Step {state.global_step}: Loss: {logs.get('loss', 'N/A')}")
                    
            except Exception as e:
                print(f"âš ï¸  SwanLab logging error for {self.model_name}: {e}")
    
    def on_evaluate(self, args, state, control, logs=None, **kwargs):
        """åœ¨è¯„ä¼°æ—¶è°ƒç”¨"""
        if self.use_swanlab and logs:
            try:
                import swanlab
                eval_dict = {}
                for k, v in logs.items():
                    if k.startswith('eval_'):
                        # åˆ†ç»„å­˜å‚¨è¯„ä¼°æŒ‡æ ‡
                        eval_dict[f'TransformerModels/{self.model_name}/{k}'] = v
                        eval_dict[f'ModelComparison/TransformerModels/{self.model_name}_{k}'] = v
                
                if eval_dict:
                    swanlab.log(eval_dict)
                    print(f"ğŸ“Š {self.model_name} Evaluation: {eval_dict}")
            except Exception as e:
                print(f"âš ï¸  SwanLab eval logging error for {self.model_name}: {e}")

def set_environment():
    """Set environment variables for GPU usage."""
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    # å¯ç”¨æ‰€æœ‰8å¼ GPUç”¨äºDLè®­ç»ƒ
    os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
    os.environ["NCCL_P2P_DISABLE"] = "1"
    os.environ["NCCL_IB_DISABLE"] = "1"
    # å¤šGPUé€šä¿¡ä¼˜åŒ–
    os.environ["NCCL_DEBUG"] = "INFO"
    os.environ["NCCL_SOCKET_IFNAME"] = "lo"
    
    print("ğŸš€ å¯ç”¨8å¼ RTX 4090 GPUè¿›è¡ŒDLè®­ç»ƒ!")
    
    # Force use of mirror for transformers library (additional method)
    try:
        from transformers import file_utils
        file_utils.HUGGINGFACE_CO_URL_HOME = "https://hf-mirror.com"
        print("Set transformers to use mirror: https://hf-mirror.com")
    except:
        try:
            import transformers.utils.hub as hub_utils
            hub_utils.HUGGINGFACE_CO_URL_HOME = "https://hf-mirror.com"
            print("Set transformers hub to use mirror")
        except:
            print("Could not set transformers mirror, using environment variables only")

def get_optimal_dl_config():
    """è·å–DLé˜¶æ®µæœ€ä¼˜å¹¶è¡Œé…ç½®"""
    try:
        import torch
        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    except ImportError:
        gpu_count = 0
    
    cpu_count = mp.cpu_count()
    
    # vLLMé£æ ¼çš„åŠ¨æ€é…ç½® - ä¼˜åŒ–8GPUé…ç½®
    if gpu_count >= 8:
        tensor_parallel_size = 8  # ä½¿ç”¨æ‰€æœ‰8å¼ GPU
        optimal_batch_size = 16 * gpu_count  # 8GPU: 128 batch size
        worker_multiplier = 4
    elif gpu_count >= 4:
        tensor_parallel_size = 4
        optimal_batch_size = 24 * gpu_count  # 4GPU: 96 batch size  
        worker_multiplier = 3
    elif gpu_count >= 2:
        tensor_parallel_size = 2
        optimal_batch_size = 32 * gpu_count  # 2GPU: 64 batch size
        worker_multiplier = 2
    else:
        tensor_parallel_size = 1
        optimal_batch_size = 16
        worker_multiplier = 1
    
    optimal_workers = min(MAX_PARALLEL_WORKERS * 2, cpu_count // 2) * worker_multiplier  # å¢åŠ workeræ•°é‡
    
    print(f"ğŸš€ vLLMé£æ ¼8GPUä¼˜åŒ–é…ç½®:")
    print(f"   - æ£€æµ‹GPUæ•°é‡: {gpu_count}")
    print(f"   - å¼ é‡å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
    print(f"   - ä¼˜åŒ–batch size: {optimal_batch_size}")
    print(f"   - ä¼˜åŒ–workers: {optimal_workers}")
    
    return {
        "gpu_count": gpu_count,
        "tensor_parallel_size": tensor_parallel_size,
        "optimal_batch_size": optimal_batch_size,
        "optimal_workers": optimal_workers,
        "enable_mixed_precision": gpu_count > 0,  # GPUå¯ç”¨æ—¶å¯ç”¨æ··åˆç²¾åº¦
        "enable_gradient_checkpointing": True,    # å†…å­˜ä¼˜åŒ–
        "enable_8gpu_optimization": gpu_count >= 8,  # 8GPUç‰¹æ®Šä¼˜åŒ–æ ‡è®°
    }

def main():
    """ä¸»è®­ç»ƒå‡½æ•° - ä½¿ç”¨å…¨å±€å¤šGPUé…ç½®å’Œå®Œæ•´è®­ç»ƒæµç¨‹"""
    # Set environment variables
    set_environment()
    
    # åˆå§‹åŒ–DLé˜¶æ®µçš„å¤šGPUé…ç½®
    dl_gpu_config = initialize_multi_gpu_for_stage("DL")
    
    # Disable wandb completely and enable swanlab
    os.environ["WANDB_DISABLED"] = "true"
    os.environ["WANDB_MODE"] = "disabled"
    os.environ["WANDB_SILENT"] = "true"
    
    print("ğŸ¯ AdvTG-DL å¤šGPUè®­ç»ƒ")
    print("=" * 60)
    print(f"ğŸ”§ DLé˜¶æ®µGPUé…ç½®:")
    print(f"   - GPUæ•°é‡: {dl_gpu_config['gpu_count']}")
    print(f"   - æ¯è®¾å¤‡batch size: {dl_gpu_config['per_device_batch_size']}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {dl_gpu_config['gradient_accumulation_steps']}")
    print(f"   - æ€»æœ‰æ•ˆbatch size: {dl_gpu_config['effective_batch_size']}")
    print(f"   - æ•°æ®åŠ è½½workers: {dl_gpu_config['dataloader_num_workers']}")
    print(f"   - æ··åˆç²¾åº¦: {dl_gpu_config['enable_mixed_precision']}")
    
    # Initialize SwanLab for experiment tracking
    try:
        import swanlab
        import time
        # åˆ›å»ºåŒ…å«æ—¶é—´æˆ³çš„è‡ªå®šä¹‰å®éªŒåç§°
        experiment_name = f"AdvTG-DL-vLLM-{time.strftime('%Y%m%d-%H%M%S')}"
        run = swanlab.init(
            project="AdvTG-DL-Training",
            name=experiment_name,  # è‡ªå®šä¹‰å®éªŒåç§°
            description="Deep Learning stage - BERT and Custom Models Training with multi-GPU optimization",
            config={
                # ç§»é™¤å­—ç¬¦ä¸²ç±»å‹å­—æ®µï¼ŒSwanLab configä¸­åªä¿ç•™æ•°å€¼ç±»å‹
                "batch_size": dl_gpu_config['per_device_batch_size'],
                "learning_rate": 2e-5,
                "num_train_epochs": 3,
                "warmup_steps": 500,
                "gpu_count": dl_gpu_config['gpu_count'],
                "effective_batch_size": dl_gpu_config['effective_batch_size'],
                "mixed_precision": 1 if dl_gpu_config['enable_mixed_precision'] else 0,  # è½¬æ¢ä¸ºæ•°å€¼
                "parallel_workers": dl_gpu_config['dataloader_num_workers'],
                "gradient_accumulation": dl_gpu_config['gradient_accumulation_steps'],
                "stage": "DL"  # ç”¨å­—ç¬¦ä¸²æ ‡è¯†è®­ç»ƒé˜¶æ®µ
            }
        )
        
        print("âœ… SwanLab initialized successfully!")
        print(f"ğŸ“Š Project: AdvTG-DL-Training")
        print(f"ğŸ“Š å­¦ä¹ ç‡ä¸º: {run.config.learning_rate}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°ä¸º: {run.config.batch_size}")
        print(f"ğŸ“Š è®­ç»ƒè½®æ•°ä¸º: {run.config.num_epochs}")
        use_swanlab = True
    except ImportError:
        print("âš ï¸  SwanLab not installed, continuing without experiment tracking")
        use_swanlab = False
    except Exception as e:
        print(f"âš ï¸  SwanLab initialization failed: {e}, continuing without experiment tracking")
        use_swanlab = False
    
    # Configuration
    FORCE_SIMPLE_MODE = False  # Set to True to skip BERT and only use custom models
    
    # Define constants (ä½¿ç”¨vLLMä¼˜åŒ–çš„å‚æ•°)
    DATA_PATH = "../dataset/dl_train.json"  # Use small dataset for DL training
    MAX_LENGTH = 512
    BATCH_SIZE = dl_gpu_config['per_device_batch_size']  # ä½¿ç”¨å¤šGPUä¼˜åŒ–çš„batch size
    LEARNING_RATE = 2e-5
    NUM_EPOCHS = 6  # å¢åŠ åˆ°6ä¸ªepochï¼Œè§‚å¯Ÿæ”¶æ•›æƒ…å†µ
    
    # Set device with vLLM-style optimization
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")
    
    # vLLMé£æ ¼çš„CUDAä¼˜åŒ–
    if torch.cuda.is_available() and ENABLE_VLLM_STYLE_PARALLEL:
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        if dl_gpu_config['gpu_count'] > 1:
            torch.cuda.set_device(0)  # è®¾ç½®ä¸»GPU
            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(dl_gpu_config['gpu_count'])))
            print(f"ğŸš€ å¯ç”¨å¤šGPUå¹¶è¡Œä¼˜åŒ–")
    
    # Load data
    print("ğŸ“ Loading data...")
    json_data = load_data(DATA_PATH)
    
    # Create models directory
    os.makedirs('../models', exist_ok=True)
    os.makedirs('../models/custom_models', exist_ok=True)
    
    # Train transformer model (BERT)
    print("\n====== Training BERT Model ======")
    
    if FORCE_SIMPLE_MODE:
        print("Force simple mode enabled - using SimpleTokenizer")
        tokenizer = SimpleTokenizer()
        print(f"Tokenizer type: {type(tokenizer)}")
        print(f"Is SimpleTokenizer: {isinstance(tokenizer, SimpleTokenizer)}")
    else:
        # Download from mirror
        transformer_model_name = "bert-base-uncased"
        print(f"Model endpoint: {os.environ.get('HF_ENDPOINT', 'https://huggingface.co')}")
        
        tokenizer = load_tokenizer(transformer_model_name)
        print(f"Tokenizer type: {type(tokenizer)}")
        print(f"Is SimpleTokenizer: {isinstance(tokenizer, SimpleTokenizer)}")
        
    # Load and prepare data
    train_dataset, val_dataset, test_dataset = prepare_dataset(json_data, tokenizer, MAX_LENGTH)
    
    # Check if using simple tokenizer and adjust training accordingly
    if hasattr(tokenizer, 'vocab_size'):
        vocab_size = tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else len(tokenizer.vocab)
    else:
        vocab_size = 30522  # BERT default vocab size
        
    # Define training arguments for transformer with å¤šGPUä¼˜åŒ–
    transformer_training_args_base = {
        "output_dir": "./models/bert",
        "per_device_train_batch_size": BATCH_SIZE,
        "per_device_eval_batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE,
        "num_train_epochs": NUM_EPOCHS,
        "eval_strategy": "epoch",  # Changed from evaluation_strategy
        "save_strategy": "epoch",
        "load_best_model_at_end": True,
        "metric_for_best_model": "eval_loss",  # ä½¿ç”¨éªŒè¯æŸå¤±ä½œä¸ºæœ€ä½³æ¨¡å‹æŒ‡æ ‡
        "greater_is_better": False,  # æŸå¤±è¶Šå°è¶Šå¥½
        "report_to": "none",  # Disable all automatic logging including wandb
        "logging_steps": 50,  # Log every 50 steps for manual tracking
    }
    
    # åˆå¹¶å…¨å±€å¤šGPUé…ç½®
    transformer_training_args_kwargs = get_training_arguments_for_stage("DL", transformer_training_args_base)
    transformer_training_args = TrainingArguments(**transformer_training_args_kwargs)
        
    # Only train transformer model if we have real transformers tokenizer
    all_model_configs = []  # æ”¶é›†æ‰€æœ‰æ¨¡å‹é…ç½®
    
    if not isinstance(tokenizer, SimpleTokenizer):
        try:
            # åˆ›å»ºBERTä¸“ç”¨çš„SwanLabå›è°ƒå’Œæ—©åœå›è°ƒ
            bert_callback = DLSwanLabCallback(use_swanlab=use_swanlab, model_name="BERT") if use_swanlab else None
            early_stopping_callback = EarlyStoppingCallback(
                early_stopping_patience=3,
                early_stopping_threshold=0.01
            )
            
            print("ğŸ›‘ BERT Early stopping enabled: patience=3, threshold=0.01")
            
            bert_save_path, bert_config = train_transformer_model(
                transformer_model_name,
                transformer_model_name,  # model_path parameter 
                train_dataset, 
                val_dataset, 
                transformer_training_args,
                swanlab_callback=bert_callback,  # ä¼ é€’å›è°ƒå‡½æ•°
                early_stopping_callback=early_stopping_callback  # ä¼ é€’æ—©åœå›è°ƒ
            )
            all_model_configs.append(bert_config)
            print("âœ… BERT model training completed!")
            
            # è®°å½•BERTæ¨¡å‹çš„æœ€ç»ˆç»“æœåˆ°SwanLab
            if use_swanlab:
                try:
                    swanlab.log({
                        'BERT_final_accuracy': bert_config.get('eval_accuracy', 0),
                        'BERT_final_precision': bert_config.get('eval_precision', 0),
                        'BERT_final_recall': bert_config.get('eval_recall', 0),
                        'BERT_final_f1': bert_config.get('eval_f1', 0),
                        'BERT_final_loss': bert_config.get('eval_loss', 0),
                        'transformer_model_completed': 1  # ç”¨æ•°å€¼è¡¨ç¤ºå®ŒæˆçŠ¶æ€
                    })
                    print(f"ğŸ“Š BERT final results logged to SwanLab")
                except Exception as e:
                    print(f"âš ï¸  SwanLab logging failed for BERT: {e}")
                    
        except Exception as e:
            print(f"âŒ BERT training failed: {e}")
            print("Continuing with custom models only...")
    else:
        print("Skipping BERT training - using simple tokenizer mode")
        print("Training custom models only...")
        
    # Train custom models
    embed_size = 128
    num_classes = 2
    
    # Define custom models
    models = {
        "textcnn": TextCNNClassifier(vocab_size, embed_size, num_classes, MAX_LENGTH),
        "cnn_lstm": CNNLSTMClassifier(vocab_size, embed_size, num_classes, MAX_LENGTH),
        "dnn": DNNClassifier(vocab_size, embed_size, num_classes, MAX_LENGTH),
        "deeplog": DeepLog(vocab_size, embed_size, num_classes, MAX_LENGTH)
    }
    
    # è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒå‚æ•° - ä½¿ç”¨å…¨å±€å¤šGPUé…ç½®
    custom_training_args_base = {
        "output_dir": "../models/custom_models",
        "per_device_train_batch_size": BATCH_SIZE,
        "per_device_eval_batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE,
        "num_train_epochs": NUM_EPOCHS,
        "report_to": "none",  # Disable all automatic logging including wandb
        "logging_steps": 50,  # Log every 50 steps for manual tracking
    }
    
    # åˆå¹¶å…¨å±€å¤šGPUé…ç½®
    custom_training_args_kwargs = get_training_arguments_for_stage("DL", custom_training_args_base)
    custom_training_args = TrainingArguments(**custom_training_args_kwargs)
    
    print("\n====== Training Custom Models ======")
    
    for model_name, model in models.items():
        print(f"\nğŸ”„ Training model: {model_name}")
        try:
            # Pass SwanLab run object to enable real-time logging
            swanlab_run_obj = run if use_swanlab else None
            save_path, model_config = train_custom_model(
                model, 
                model_name, 
                train_dataset, 
                val_dataset, 
                custom_training_args,
                swanlab_run=swanlab_run_obj
            )
            all_model_configs.append(model_config)
            print(f"âœ… Completed training model: {model_name}")
            
            # è®°å½•è‡ªå®šä¹‰æ¨¡å‹çš„æœ€ç»ˆç»“æœåˆ°SwanLab
            if use_swanlab:
                try:
                    swanlab.log({
                        f'{model_name}_final_accuracy': model_config.get('accuracy', 0),
                        f'{model_name}_final_precision': model_config.get('precision', 0),
                        f'{model_name}_final_recall': model_config.get('recall', 0),
                        f'{model_name}_final_f1': model_config.get('f1', 0),
                        # ç§»é™¤å­—ç¬¦ä¸²ç±»å‹å­—æ®µï¼ŒSwanLabæœŸæœ›æ•°å€¼ç±»å‹
                        'custom_model_completed': 1  # ç”¨æ•°å€¼è¡¨ç¤ºå®ŒæˆçŠ¶æ€
                    })
                    print(f"ğŸ“Š {model_name} final results logged to SwanLab")
                except Exception as e:
                    print(f"âš ï¸  SwanLab logging failed for {model_name}: {e}")
            
        except Exception as e:
            print(f"âŒ Failed to train {model_name}: {e}")
    
    # Save model configurations for RL stage
    import pickle
    config_save_path = "../models/model_configs.pkl"
    os.makedirs(os.path.dirname(config_save_path), exist_ok=True)
    
    with open(config_save_path, 'wb') as f:
        pickle.dump(all_model_configs, f)
    
    # Create image model configurations (placeholder)
    try:
        from create_image_configs import create_image_model_configs
        create_image_model_configs()
        print("âœ… Created image model configurations")
    except Exception as e:
        print(f"âš ï¸  Failed to create image model configs: {e}")
    
    # Log results to SwanLab if available
    if use_swanlab:
        try:
            # Log model performance metrics
            for i, config in enumerate(all_model_configs):
                model_name = config.get('model_name', f'model_{i}')
                if 'accuracy' in config:
                    swanlab.log({f"{model_name}/accuracy": config['accuracy']})
                if 'f1' in config:
                    swanlab.log({f"{model_name}/f1_score": config['f1']})
                if 'precision' in config:
                    swanlab.log({f"{model_name}/precision": config['precision']})
                if 'recall' in config:
                    swanlab.log({f"{model_name}/recall": config['recall']})
            
            # Log summary metrics
            swanlab.log({
                "total_models_trained": len(all_model_configs),
                "training_completed": 1,
                "multi_gpu_optimization_enabled": 1 if dl_gpu_config['gpu_count'] > 1 else 0,  # è½¬æ¢ä¸ºæ•°å€¼
                "final_gpu_count": dl_gpu_config['gpu_count'],
                "final_batch_size": BATCH_SIZE,
                "final_workers": dl_gpu_config['dataloader_num_workers'],
                "final_effective_batch_size": dl_gpu_config['effective_batch_size']
            })
            
            swanlab.finish()
            print("ğŸ“Š Results logged to SwanLab successfully!")
        except Exception as e:
            print(f"âš ï¸  SwanLab logging failed: {e}")
    
    print(f"\nâœ… Saved text model configurations to: {config_save_path}")
    print(f"ğŸ“Š Total text models configured: {len(all_model_configs)}")
    
    print("\nğŸ‰ All models training completed with multi-GPU optimization!")

if __name__ == "__main__":
    main()
