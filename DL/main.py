import os
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import time

# vLLMé£æ ¼çš„DLå¹¶è¡Œè®­ç»ƒé…ç½®
ENABLE_VLLM_STYLE_PARALLEL = True   # å¯ç”¨vLLMé£æ ¼å¹¶è¡Œä¼˜åŒ–
ENABLE_TENSOR_PARALLEL = True       # å¯ç”¨å¼ é‡å¹¶è¡Œï¼ˆå¤šGPUï¼‰
ENABLE_DATA_PARALLEL = True         # å¯ç”¨æ•°æ®å¹¶è¡Œå¤„ç†
MAX_PARALLEL_WORKERS = min(6, mp.cpu_count())  # å¢åŠ å¹¶è¡Œå·¥ä½œè¿›ç¨‹

def get_optimal_dl_config():
    """è·å–DLé˜¶æ®µæœ€ä¼˜å¹¶è¡Œé…ç½®"""
    try:
        import torch
        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    except ImportError:
        gpu_count = 0
    
    cpu_count = mp.cpu_count()
    
    # vLLMé£æ ¼çš„åŠ¨æ€é…ç½®
    if gpu_count >= 2:
        tensor_parallel_size = min(4, gpu_count)
        optimal_batch_size = 32 * gpu_count  # æ ¹æ®GPUæ•°é‡æ‰©å±•
        worker_multiplier = 2
    else:
        tensor_parallel_size = 1
        optimal_batch_size = 16
        worker_multiplier = 1
    
    optimal_workers = min(MAX_PARALLEL_WORKERS, cpu_count // 2) * worker_multiplier
    
    return {
        "gpu_count": gpu_count,
        "tensor_parallel_size": tensor_parallel_size,
        "optimal_batch_size": optimal_batch_size,
        "optimal_workers": optimal_workers,
        "enable_mixed_precision": gpu_count > 0,  # GPUå¯ç”¨æ—¶å¯ç”¨æ··åˆç²¾åº¦
        "enable_gradient_checkpointing": True,    # å†…å­˜ä¼˜åŒ–
    }

def create_swanlab_logger():
    """åˆ›å»ºSwanLabæ—¥å¿—è®°å½•å™¨ - ç±»ä¼¼ç¤ºä¾‹ä»£ç çš„æ¨¡å¼"""
    try:
        import swanlab
        import random
        
        # è·å–vLLMé…ç½®
        vllm_config = get_optimal_dl_config()
        
        # ç±»ä¼¼ç¤ºä¾‹ä»£ç çš„åˆå§‹åŒ–
        run = swanlab.init(
            project="AdvTG-DL-Training",
            config={
                "framework": "AdvTG-DL",
                "optimization": "vLLM-style", 
                "learning_rate": 2e-5,
                "epochs": 3,
                "batch_size": vllm_config['optimal_batch_size'],
                "max_length": 512,
                "gpu_count": vllm_config['gpu_count'],
                "tensor_parallel_size": vllm_config['tensor_parallel_size'],
                "mixed_precision": vllm_config['enable_mixed_precision'],
                "parallel_workers": vllm_config['optimal_workers']
            }
        )
        
        print(f"âœ… SwanLab initialized successfully!")
        print(f"ğŸ“Š Project: {run.project}")
        print(f"ğŸ“Š å­¦ä¹ ç‡ä¸º: {run.config.learning_rate}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°ä¸º: {run.config.batch_size}")
        print(f"ğŸ“Š è®­ç»ƒè½®æ•°ä¸º: {run.config.epochs}")
        
        return run, True
        
    except ImportError:
        print("âš ï¸  SwanLab not installed, continuing without experiment tracking")
        return None, False
    except Exception as e:
        print(f"âš ï¸  SwanLab initialization failed: {e}")
        return None, False

def log_training_progress(epoch, step, loss, accuracy=None, model_name="DL"):
    """è®°å½•è®­ç»ƒè¿›åº¦ - ç±»ä¼¼ç¤ºä¾‹ä»£ç çš„logæ¨¡å¼"""
    try:
        import swanlab
        import random
        
        # ç±»ä¼¼ç¤ºä¾‹ä»£ç çš„æ—¥å¿—è®°å½•
        log_data = {
            "epoch": epoch,
            "step": step,
            "loss": loss,
            "model": model_name
        }
        
        if accuracy is not None:
            log_data["accuracy"] = accuracy
            
        # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„åŠ¨æ€å˜åŒ–ï¼ˆç±»ä¼¼ç¤ºä¾‹ä»£ç ï¼‰
        offset = random.random() / 5
        log_data["learning_rate_dynamic"] = 2e-5 * (1 - epoch * 0.1) + offset
        
        swanlab.log(log_data)
        print(f"ğŸ“Š {model_name} - epoch={epoch}, step={step}, loss={loss:.4f}")
        
        if accuracy is not None:
            print(f"ğŸ“Š {model_name} - accuracy={accuracy:.4f}")
            
    except Exception as e:
        print(f"âš ï¸  SwanLab logging failed: {e}")

def simulate_training_with_swanlab():
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå±•ç¤ºSwanLabé›†æˆ - åŸºäºç¤ºä¾‹ä»£ç æ¨¡å¼"""
    
    # åˆå§‹åŒ–SwanLab
    run, use_swanlab = create_swanlab_logger()
    
    if not use_swanlab:
        print("âŒ SwanLabæœªå¯ç”¨ï¼Œæ— æ³•å±•ç¤ºå›¾è¡¨")
        return
    
    print("\nğŸš€ å¼€å§‹æ¨¡æ‹ŸvLLMé£æ ¼DLè®­ç»ƒè¿‡ç¨‹...")
    print("=" * 60)
    
    # è·å–é…ç½®
    import random
    vllm_config = get_optimal_dl_config()
    
    # æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„è®­ç»ƒ
    models = ["BERT", "TextCNN", "CNN-LSTM", "DNN", "DeepLog"]
    
    for model_name in models:
        print(f"\nğŸ”„ è®­ç»ƒæ¨¡å‹: {model_name}")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆç±»ä¼¼ç¤ºä¾‹ä»£ç ï¼‰
        offset = random.random() / 5
        
        for epoch in range(1, run.config.epochs + 1):
            for step in range(1, 11):  # æ¯ä¸ªepoch 10ä¸ªstep
                # ç±»ä¼¼ç¤ºä¾‹ä»£ç çš„æŸå¤±å’Œå‡†ç¡®ç‡è®¡ç®—
                loss = 2**(-epoch) + random.random() / epoch + offset
                acc = 1 - 2**(-epoch) - random.random() / epoch - offset
                
                # ç¡®ä¿å‡†ç¡®ç‡åœ¨åˆç†èŒƒå›´
                acc = max(0.5, min(0.99, acc))
                
                # è®°å½•åˆ°SwanLab
                log_training_progress(epoch, step, loss, acc, model_name)
                
                # æ¨¡æ‹Ÿè®­ç»ƒå»¶è¿Ÿ
                time.sleep(0.1)
        
        print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ")
    
    # æœ€ç»ˆæ€»ç»“
    try:
        import swanlab
        swanlab.log({
            "training_completed": 1,
            "total_models_trained": len(models),
            "vllm_optimization_enabled": ENABLE_VLLM_STYLE_PARALLEL,
            "final_gpu_count": vllm_config['gpu_count']
        })
        print("ğŸ“Š è®­ç»ƒæ€»ç»“å·²è®°å½•åˆ°SwanLab")
    except Exception as e:
        print(f"âš ï¸  æœ€ç»ˆæ—¥å¿—è®°å½•å¤±è´¥: {e}")
    
    print("\nâœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print("ğŸ“Š è¯·æŸ¥çœ‹SwanLabé¢æ¿æŸ¥çœ‹è®­ç»ƒå›¾è¡¨å’Œè¿›åº¦")

if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒ
    os.environ["WANDB_DISABLED"] = "true"
    os.environ["WANDB_MODE"] = "disabled"
    
    print("ğŸ¯ AdvTG-DL vLLMé£æ ¼å¹¶è¡Œè®­ç»ƒæ¼”ç¤º")
    print("ğŸ“Š SwanLabé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    config = get_optimal_dl_config()
    print(f"ğŸ”§ vLLMé£æ ¼é…ç½®:")
    print(f"   - GPUæ•°é‡: {config['gpu_count']}")
    print(f"   - å¼ é‡å¹¶è¡Œå¤§å°: {config['tensor_parallel_size']}")
    print(f"   - ä¼˜åŒ–batch size: {config['optimal_batch_size']}")
    print(f"   - ä¼˜åŒ–workers: {config['optimal_workers']}")
    print(f"   - æ··åˆç²¾åº¦: {config['enable_mixed_precision']}")
    
    # è¿è¡Œè®­ç»ƒæ¨¡æ‹Ÿ
    simulate_training_with_swanlab()
