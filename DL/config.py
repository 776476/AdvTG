"""
DLé˜¶æ®µé…ç½®æ–‡ä»¶
ç»Ÿä¸€ç®¡ç†æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ‰€æœ‰é…ç½®å‚æ•°
"""
import os
import multiprocessing as mp
import torch

class DLConfig:
    """DLé˜¶æ®µé…ç½®ç±»"""
    
    def __init__(self, dl_gpu_config=None):
        self.dl_gpu_config = dl_gpu_config or {}
        
        # åŸºç¡€é…ç½®
        self.DATA_PATH = "../dataset/dl_train.json"
        self.MAX_LENGTH = 512
        self.LEARNING_RATE = 2e-5
        self.NUM_EPOCHS = 6
        self.FORCE_SIMPLE_MODE = False
        
        # GPUç›¸å…³é…ç½®
        self.BATCH_SIZE = self.dl_gpu_config.get('per_device_batch_size', 8)
        self.GPU_COUNT = self.dl_gpu_config.get('gpu_count', 1)
        
        # vLLMé£Žæ ¼å¹¶è¡Œé…ç½®
        self.ENABLE_VLLM_STYLE_PARALLEL = True
        self.ENABLE_TENSOR_PARALLEL = True
        self.ENABLE_DATA_PARALLEL = True
        self.MAX_PARALLEL_WORKERS = min(6, mp.cpu_count())
        
        # æ¨¡åž‹é…ç½®
        self.EMBED_SIZE = 128
        self.NUM_CLASSES = 2
        self.VOCAB_SIZE = 30522  # BERT default
        
        # è·¯å¾„é…ç½®
        self.MODEL_SAVE_DIR = '../models'
        self.CUSTOM_MODEL_DIR = '../models/custom_models'
        self.CONFIG_SAVE_PATH = "../models/model_configs.pkl"
        
        # è®­ç»ƒå™¨é…ç½®
        self.BERT_MODEL_NAME = "bert-base-uncased"
        self.EARLY_STOPPING_PATIENCE = 3
        self.EARLY_STOPPING_THRESHOLD = 0.01
        self.LOGGING_STEPS = 50
        
        # SwanLabé…ç½®
        self.SWANLAB_PROJECT = "AdvTG-DL-Training"
        self.SWANLAB_DESCRIPTION = "Deep Learning stage - BERT and Custom Models Training with multi-GPU optimization"
    
    def get_transformer_training_args(self):
        """èŽ·å–Transformeræ¨¡åž‹è®­ç»ƒå‚æ•°"""
        return {
            "output_dir": "./models/bert",
            "per_device_train_batch_size": self.BATCH_SIZE,
            "per_device_eval_batch_size": self.BATCH_SIZE,
            "learning_rate": self.LEARNING_RATE,
            "num_train_epochs": self.NUM_EPOCHS,
            "eval_strategy": "epoch",
            "save_strategy": "epoch",
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            "report_to": "none",
            "logging_steps": self.LOGGING_STEPS,
            # GPUç›¸å…³é…ç½®
            "dataloader_num_workers": self.dl_gpu_config.get('dataloader_num_workers', 4),
            "dataloader_pin_memory": self.dl_gpu_config.get('dataloader_pin_memory', True),
            "fp16": self.dl_gpu_config.get('enable_mixed_precision', False),
            "gradient_accumulation_steps": self.dl_gpu_config.get('gradient_accumulation_steps', 1),
        }
    
    def get_custom_training_args(self):
        """èŽ·å–è‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒå‚æ•°"""
        return {
            "output_dir": self.CUSTOM_MODEL_DIR,
            "per_device_train_batch_size": self.BATCH_SIZE,
            "per_device_eval_batch_size": self.BATCH_SIZE,
            "learning_rate": self.LEARNING_RATE,
            "num_train_epochs": self.NUM_EPOCHS,
            "report_to": "none",
            "logging_steps": self.LOGGING_STEPS,
            # GPUç›¸å…³é…ç½®
            "dataloader_num_workers": self.dl_gpu_config.get('dataloader_num_workers', 4),
            "dataloader_pin_memory": self.dl_gpu_config.get('dataloader_pin_memory', True),
            "fp16": self.dl_gpu_config.get('enable_mixed_precision', False),
            "gradient_accumulation_steps": self.dl_gpu_config.get('gradient_accumulation_steps', 1),
        }
    
    def get_swanlab_config(self):
        """èŽ·å–SwanLabé…ç½®"""
        return {
            "batch_size": self.BATCH_SIZE,
            "learning_rate": self.LEARNING_RATE,
            "num_train_epochs": self.NUM_EPOCHS,
            "warmup_steps": 500,
            "gpu_count": self.GPU_COUNT,
            "effective_batch_size": self.dl_gpu_config.get('effective_batch_size', self.BATCH_SIZE),
            "mixed_precision": 1 if self.dl_gpu_config.get('enable_mixed_precision', False) else 0,
            "parallel_workers": self.dl_gpu_config.get('dataloader_num_workers', 4),
            "gradient_accumulation": self.dl_gpu_config.get('gradient_accumulation_steps', 1),
            "stage": "DL"
        }
    
    def get_model_definitions(self):
        """èŽ·å–è‡ªå®šä¹‰æ¨¡åž‹å®šä¹‰"""
        from models import CNNLSTMClassifier, TextCNNClassifier, DNNClassifier, DeepLog
        
        return {
            "textcnn": TextCNNClassifier(self.VOCAB_SIZE, self.EMBED_SIZE, self.NUM_CLASSES, self.MAX_LENGTH),
            "cnn_lstm": CNNLSTMClassifier(self.VOCAB_SIZE, self.EMBED_SIZE, self.NUM_CLASSES, self.MAX_LENGTH),
            "dnn": DNNClassifier(self.VOCAB_SIZE, self.EMBED_SIZE, self.NUM_CLASSES, self.MAX_LENGTH),
            "deeplog": DeepLog(self.VOCAB_SIZE, self.EMBED_SIZE, self.NUM_CLASSES, self.MAX_LENGTH)
        }
    
    def setup_environment(self):
        """è®¾ç½®çŽ¯å¢ƒå˜é‡å’ŒGPUé…ç½®"""
        # CUDAé…ç½®
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
        os.environ["NCCL_P2P_DISABLE"] = "1"
        os.environ["NCCL_IB_DISABLE"] = "1"
        os.environ["NCCL_DEBUG"] = "INFO"
        os.environ["NCCL_SOCKET_IFNAME"] = "lo"
        
        # Wandbç¦ç”¨
        os.environ["WANDB_DISABLED"] = "true"
        os.environ["WANDB_MODE"] = "disabled"
        os.environ["WANDB_SILENT"] = "true"
        
        # Tokenizersé…ç½®
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        # Hugging Faceé•œåƒ
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        os.environ["HUGGINGFACE_HUB_ENDPOINT"] = "https://hf-mirror.com"
        os.environ["HF_HUB_ENDPOINT"] = "https://hf-mirror.com"
        os.environ["HUGGINGFACE_HUB_URL"] = "https://hf-mirror.com"
        
        print("ðŸš€ å¯ç”¨8å¼ RTX 4090 GPUè¿›è¡ŒDLè®­ç»ƒ!")
        
        # è®¾ç½®transformersé•œåƒ
        self._setup_transformers_mirror()
    
    def _setup_transformers_mirror(self):
        """è®¾ç½®transformersåº“é•œåƒ"""
        try:
            from transformers import file_utils
            file_utils.HUGGINGFACE_CO_URL_HOME = "https://hf-mirror.com"
            print("Set transformers to use mirror: https://hf-mirror.com")
        except:
            try:
                import transformers.utils.hub as hub_utils
                hub_utils.HUGGINGFACE_CO_URL_HOME = "https://hf-mirror.com"
                print("Set transformers hub to use mirror")
            except:
                print("Could not set transformers mirror, using environment variables only")
    
    def setup_gpu_optimization(self):
        """è®¾ç½®GPUä¼˜åŒ–"""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ðŸŽ® ä½¿ç”¨è®¾å¤‡: {device}")
        
        if torch.cuda.is_available() and self.ENABLE_VLLM_STYLE_PARALLEL:
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            if self.GPU_COUNT > 1:
                torch.cuda.set_device(0)
                os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(self.GPU_COUNT)))
                print(f"ðŸš€ å¯ç”¨å¤šGPUå¹¶è¡Œä¼˜åŒ–")
        
        return device
    
    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(self.MODEL_SAVE_DIR, exist_ok=True)
        os.makedirs(self.CUSTOM_MODEL_DIR, exist_ok=True)
        os.makedirs(os.path.dirname(self.CONFIG_SAVE_PATH), exist_ok=True)
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("ðŸŽ¯ AdvTG-DL å¤šGPUè®­ç»ƒ")
        print("=" * 60)
        print(f"ðŸ”§ DLé˜¶æ®µé…ç½®æ‘˜è¦:")
        print(f"   - GPUæ•°é‡: {self.GPU_COUNT}")
        print(f"   - æ¯è®¾å¤‡batch size: {self.BATCH_SIZE}")
        print(f"   - å­¦ä¹ çŽ‡: {self.LEARNING_RATE}")
        print(f"   - è®­ç»ƒè½®æ•°: {self.NUM_EPOCHS}")
        print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {self.MAX_LENGTH}")
        print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.dl_gpu_config.get('gradient_accumulation_steps', 1)}")
        print(f"   - æ€»æœ‰æ•ˆbatch size: {self.dl_gpu_config.get('effective_batch_size', self.BATCH_SIZE)}")
        print(f"   - æ•°æ®åŠ è½½workers: {self.dl_gpu_config.get('dataloader_num_workers', 4)}")
        print(f"   - æ··åˆç²¾åº¦: {self.dl_gpu_config.get('enable_mixed_precision', False)}")
        print(f"   - vLLMé£Žæ ¼ä¼˜åŒ–: {self.ENABLE_VLLM_STYLE_PARALLEL}")


def get_optimal_dl_config():
    """èŽ·å–DLé˜¶æ®µæœ€ä¼˜å¹¶è¡Œé…ç½® - ç‹¬ç«‹å‡½æ•°ç‰ˆæœ¬"""
    try:
        import torch
        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    except ImportError:
        gpu_count = 0
    
    cpu_count = mp.cpu_count()
    MAX_PARALLEL_WORKERS = min(6, mp.cpu_count())
    
    # vLLMé£Žæ ¼çš„åŠ¨æ€é…ç½® - ä¼˜åŒ–8GPUé…ç½®
    if gpu_count >= 8:
        tensor_parallel_size = 8
        optimal_batch_size = 16 * gpu_count
        worker_multiplier = 4
    elif gpu_count >= 4:
        tensor_parallel_size = 4
        optimal_batch_size = 24 * gpu_count
        worker_multiplier = 3
    elif gpu_count >= 2:
        tensor_parallel_size = 2
        optimal_batch_size = 32 * gpu_count
        worker_multiplier = 2
    else:
        tensor_parallel_size = 1
        optimal_batch_size = 16
        worker_multiplier = 1
    
    optimal_workers = min(MAX_PARALLEL_WORKERS * 2, cpu_count // 2) * worker_multiplier
    
    print(f"ðŸš€ vLLMé£Žæ ¼8GPUä¼˜åŒ–é…ç½®:")
    print(f"   - æ£€æµ‹GPUæ•°é‡: {gpu_count}")
    print(f"   - å¼ é‡å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
    print(f"   - ä¼˜åŒ–batch size: {optimal_batch_size}")
    print(f"   - ä¼˜åŒ–workers: {optimal_workers}")
    
    return {
        "gpu_count": gpu_count,
        "tensor_parallel_size": tensor_parallel_size,
        "optimal_batch_size": optimal_batch_size,
        "optimal_workers": optimal_workers,
        "enable_mixed_precision": gpu_count > 0,
        "enable_gradient_checkpointing": True,
        "enable_8gpu_optimization": gpu_count >= 8,
    }
